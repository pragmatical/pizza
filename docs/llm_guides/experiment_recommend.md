# Recommendations for Experimentation

Experimenting with large language models (LLMs) can be challenging, and may require careful design choices. Here are some recommendations for experimenting with LLMs effectively and efficiently.

## Collect as much good quality data as possible then augment with synthetic data as needed

Large language models are trained on massive amounts of text from diverse sources, but they may have not seen enough data for your specific domain or task. Therefore, it is important to collect as much relevant and high-quality data as possible to provide the model with sufficient context and examples. However, data collection can be costly and time-consuming, so you may also consider augmenting your data with synthetic data generated by a LLM or other generative or deterministic methods (e.g., grammar-based). Synthetic data can help increase the diversity and robustness of your data, as well as fill in the gaps or imbalances in your data distribution.

## Define/use different evaluation metrics that fit your application

When using LLMs, you need to define or use different evaluation metrics that can capture the performance of your LLM-based solution. Depending on your application, you may use automated metrics, such as Levenshtein distance, BLEU, ROUGE, or human-in-loop evaluation, such as ratings, rankings, or feedback. You may also use multiple metrics to get a comprehensive and holistic assessment of your model. Please refer to [evaluation metrics](evaluation_metrics.md) for more details.

## Start with prompt engineering

- Start simple to establish a baseline – start with simple prompt designs and use that as a baseline. This can be a quick and easy way to gauge the model's capabilities and limitations.
- Gradually increase complexity – once you have a baseline, you can experiment with increasing the complexity of your task or domain by providing more context or examples, or introducing constraints.
- Try different prompt engineering methods to optimize the performance – different prompt engineering can elicit different responses from the model, and some may be more suitable or effective for your task or domain than others. Therefore, try different prompt designs and compare their results.
- Do benchmarking using different configurations and evaluate different models – you can use different prompt designs, model parameters, datasets, metrics, etc. to benchmark the model, and see how it performs on different aspects of your task or domain. You can also evaluate different models, such as different versions or variants of GPT-3, or other large language models and see how they compare.

## Perform fine-tuning if needed

While there are use cases where [fine-tuning](./fine_tuning.md) can help improve the model's performance and adaptability, it has limitations due to the costs, the need for more data, computational resources, and hyperparameter tuning, and it may also cause over-fitting or catastrophic forgetting. Therefore, we advise doing fine-tuning only if needed, and only after you have exhausted the in-context learning methods. Below are a few recommendations for fine-tuning. Please refer to [fine-tuning recommendations](fine_tuning_recommend.md) for more details.

- Start with smaller models especially for simple tasks. Smaller models can be faster, cheaper, and easier to use and fine-tune, and they can also be more interpretable and controllable.
- Try fine-tuning using different data formats. Different data formats can affect the model's input and output representations, and some may be more suitable or effective for your task or domain than others. For example, you can use plain text, structured text, or semi-structured text as your data format, and you can use different separators, delimiters, or tokens to indicate the boundaries or labels of your input and output.
- Optimize the hyper-parameters of your model and your fine-tuning process, such as the learning rate, the batch size, the number of epochs, the weight decay, or the dropout rate.
