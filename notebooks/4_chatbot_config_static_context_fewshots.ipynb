{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A pizza order bot using config 02\n",
    "\n",
    "This sample notebook demonstrates  creation of a simple chatbot that uses an FFModel inference endpoint which in turn calls OpenAPI chat completions api.   The goal is to demonstrate interactions with the endpoint in a scenario that requires history being sent with the prompt.  The endpoint is designed to return json so successful results will look \"funny\" in the chat.  \n",
    "\n",
    "This configuration sends a prompt that has text from a file added to the sytem message. As well as some example prompt/response pairs (fewshot).  \n",
    "The system text is as follows:\n",
    "- *You are bot that takes orders at a pizza and wings restaurant\n",
    "- *You take orders in natural language and translate it to a formatted json object\n",
    "- *Results should be provided in a structured output that is json format that adheres to this schema\n",
    "    {\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"array\",\"description\":\"An array of items in the order\",\"items\":{\"type\":\"object\",\"oneOf\":[{\"properties\":{\"type\":{\"const\":\"specialty pizza\"},\"name\":{\"type\":\"string\",\"description\":\"The name of the pizza\"},\"size\":{\"type\":\"string\",\"description\":\"The size of the pizza\"},\"crust\":{\"type\":\"string\",\"description\":\"The type of crust for the pizza\"},\"quantity\":{\"type\":\"integer\",\"description\":\"The quantity of the pizza in the order\"}},\"required\":[\"type\",\"name\",\"quantity\",\"size\",\"crust\"]},{\"properties\":{\"type\":{\"const\":\"custom pizza\"},\"size\":{\"type\":\"string\",\"description\":\"The size of the pizza\"},\"crust\":{\"type\":\"string\",\"description\":\"The type of crust for the pizza\"},\"quantity\":{\"type\":\"integer\",\"description\":\"The quantity of the pizza in the order\"},\"toppings\":{\"type\":\"array\",\"description\":\"An array of toppings on the pizza\",\"items\":{\"type\":\"string\",\"description\":\"The name of a topping\"}}},\"required\":[\"type\",\"size\",\"crust\",\"quantity\",\"toppings\"]},{\"properties\":{\"type\":{\"const\":\"wings\"},\"name\":{\"type\":\"string\",\"description\":\"The name of the wings\"},\"quantity\":{\"type\":\"integer\",\"description\":\"The quantity of the wings in the order\"},\"flavor\":{\"type\":\"string\",\"description\":\"The flavor of the wings\"}},\"required\":[\"type\",\"name\",\"quantity\",\"flavor\"]},{\"properties\":{\"type\":{\"const\":\"drink\"},\"name\":{\"type\":\"string\",\"description\":\"The name of the drink\"},\"quantity\":{\"type\":\"integer\",\"description\":\"The quantity of the drink in the order\"},\"size\":{\"type\":\"string\",\"description\":\"The size of the drink\"}},\"required\":[\"type\",\"name\",\"quantity\",\"size\"]},{\"properties\":{\"type\":{\"const\":\"topping\"},\"name\":{\"type\":\"string\",\"description\":\"The name of the topping\"},\"quantity\":{\"type\":\"integer\",\"description\":\"The quantity of the topping in the order\"}},\"required\":[\"type\",\"name\",\"quantity\"]}]}}\n",
    "- *Only return the json part of the response\n",
    "- *If asked to order anything that is not related to pizza, wings, or drinks, you politely decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 11:16:48,303 name=ffmodel.core.inference_endpoint level=METRICS solution_config \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 4 configs from file '/Users/jorgeluna/.chatbot.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n",
      "FFModelLogger: Logging will print to console as no AppInsight connection was provided (config `ApplicationInsightConnectionString)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jorgeluna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ffmodel.core.inference_endpoint import InferenceEndpoint\n",
    "\n",
    "environment_config_path = \"~/.chatbot.env\"\n",
    "\n",
    "solution_config_path = \"../solution/configs/02_with_fewshots_01_min.yaml\"\n",
    "\n",
    "endpoint = InferenceEndpoint(solution_config_path, environment_config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define Request to OpenAI API\n",
    "For these labs we are using the Azure OpenAI Chat Completion API, this is different than the Completions API.  Information about its usage can be found here:\n",
    "<https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions>\n",
    "The chat completions API takes a collection of messages to provide history and context of chat to the model.  This function takes a chat request object that ffmodel inference endpoint can use to create a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(chat_request):\n",
    "    response = endpoint.execute(chat_request)\n",
    "    return response.model_output.completions[0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 The conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Name:  Jorge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to pizzaai.  What can I get for you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want some candy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, but I am a bot that takes orders for a pizza and wings restaurant. I cannot help you with candy orders. Is there anything related to pizza, wings, or drinks that I can assist you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want some pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Sure thing! What kind of pizza would you like?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want a large pepperoni pizza with mushrooms and a large diet coke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  {\"items\": [{\"type\": \"custom pizza\", \"size\": \"large\", \"crust\": \"hand-tossed\", \"quantity\": 1, \"toppings\": [\"pepperoni\", \"mushrooms\"]}, {\"type\": \"drink\", \"name\": \"Diet Coke\", \"quantity\": 1, \"size\": \"large\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jorge: I would like some candy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, but we only serve pizza, wings, and drinks. Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jorge: I would like to get a large pepperoni pizza with mushrooms and a 2 liter of Diet Coke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  [{\"type\": \"specialty pizza\", \"name\": \"pepperoni\", \"size\": \"large\", \"crust\": \"regular\", \"quantity\": 1}, {\"type\": \"topping\", \"name\": \"mushrooms\", \"quantity\": 1}, {\"type\": \"drink\", \"name\": \"Diet Coke\", \"size\": \"2 liter\", \"quantity\": 1}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jorge: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Bye\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "\n",
    "class CompletionPair(BaseModel):\n",
    "    user_nl:str\n",
    "    completion:object\n",
    "\n",
    "class UserSession(BaseModel):\n",
    "    session_id:str\n",
    "    prior_responses:list[CompletionPair]\n",
    "    sequence: typing.Optional[int]\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_nl:str\n",
    "    session:UserSession\n",
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "max_response_tokens = 500\n",
    "token_limit=2048\n",
    "name = input('Enter Your Name: ')\n",
    "print ('Welcome to pizzaai.  What can I get for you?')\n",
    "conversation=[]\n",
    "session_id=str(uuid.uuid4())\n",
    "message_count=0\n",
    "while True:\n",
    "    request = input(name+':')\n",
    "    current_message={request}\n",
    "    chat_request=ChatRequest(user_nl=request,session=UserSession(session_id=session_id,sequence=message_count,prior_responses=conversation))\n",
    "    if request==\"Bye\" or request=='bye':\n",
    "        print('Bot: Bye')\n",
    "        break\n",
    "    else:\n",
    "        payload=json.dumps(jsonable_encoder(chat_request))\n",
    "        response = send_message(payload)\n",
    "        interaction = CompletionPair(user_nl=request,completion=response)\n",
    "        conversation.append(interaction)\n",
    "        message_count+=1\n",
    "        print('Bot: ', response)\n",
    "        request=None\n",
    "        current_message=None\n",
    "        response=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
