{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A pizza order bot Using Config 00\n",
    "\n",
    "This sample notebook demonstrates  creation of a simple chatbot that uses an FFModel inference endpoint which in turn calls OpenAPI chat completions api.   The goal is to demonstrate interactions with the endpoint in a scenario that requires history being sent with the prompt.  The endpoint is designed to return json so successful results will look \"funny\" in the chat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 12:20:50,249 name=ffmodel.core.inference_endpoint level=METRICS solution_config \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 4 configs from file '/Users/jorgeluna/.chatbot.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n",
      "FFModelLogger: Logging will print to console as no AppInsight connection was provided (config `ApplicationInsightConnectionString)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jorgeluna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ffmodel.core.inference_endpoint import InferenceEndpoint\n",
    "\n",
    "environment_config_path = \"~/.chatbot.env\"\n",
    "\n",
    "solution_config_path = \"../solution/configs/00_baseline.yaml\"\n",
    "#solution_config_path = \"../solution/configs/01_with_static_context_01.yaml\"\n",
    "#solution_config_path = \"../solution/configs/02_with_fewshots_01 _min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/03_with_fewshots_01.yaml\"\n",
    "#solution_config_path = \"../solution/configs/05_with_fewshots_02_min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/04_with_fewshots_02.yaml\"\n",
    "\n",
    "endpoint = InferenceEndpoint(solution_config_path, environment_config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define Request to OpenAI API\n",
    "For these labs we are using the Azure OpenAI Chat Completion API, this is different than the Completions API.  Information about its usage can be found here:\n",
    "<https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions>\n",
    "The chat completions API takes a collection of messages to provide history and context of chat to the model.  This function takes a chat request object that ffmodel inference endpoint can use to create a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(chat_request):\n",
    "    response = endpoint.execute(chat_request)\n",
    "    return response.model_output.completions[0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 The conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Name:  Jorge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to pizzaai.  What can I get for you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want some candy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  As an AI language model, I cannot provide you with candy. However, you can visit a nearby store or candy shop to purchase some candy. Remember to consume it in moderation and maintain a healthy diet.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want some pizza\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, as an AI language model, I cannot provide you with pizza. However, you can order pizza from your local pizzeria or make your own pizza at home.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want a large pepperoni pizza with mushrooms and a medium diet coke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  As an AI language model, I cannot order food for you. However, you can place your order by calling your local pizza restaurant or using their online ordering system. Just make sure to specify your toppings and drink preference. Enjoy your meal!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Jorge: I want candy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  As an AI language model, I cannot provide you with candy. However, you can go to a nearby store and buy some candy or order it online. Remember to consume it in moderation and maintain a healthy diet.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "\n",
    "class CompletionPair(BaseModel):\n",
    "    user_nl:str\n",
    "    completion:object\n",
    "\n",
    "class UserSession(BaseModel):\n",
    "    session_id:str\n",
    "    prior_responses:list[CompletionPair]\n",
    "    sequence: typing.Optional[int]\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_nl:str\n",
    "    session:UserSession\n",
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "max_response_tokens = 500\n",
    "token_limit=2048\n",
    "name = input('Enter Your Name: ')\n",
    "print ('Welcome to pizzaai.  What can I get for you?')\n",
    "conversation=[]\n",
    "session_id=str(uuid.uuid4())\n",
    "message_count=0\n",
    "while True:\n",
    "    request = input(name+':')\n",
    "    current_message={request}\n",
    "    chat_request=ChatRequest(user_nl=request,session=UserSession(session_id=session_id,sequence=message_count,prior_responses=conversation))\n",
    "    if request==\"Bye\" or request=='bye':\n",
    "        print('Bot: Bye')\n",
    "        break\n",
    "    else:\n",
    "        payload=json.dumps(jsonable_encoder(chat_request))\n",
    "        response = send_message(payload)\n",
    "        interaction = CompletionPair(user_nl=request,completion=response)\n",
    "        conversation.append(interaction)\n",
    "        message_count+=1\n",
    "        print('Bot: ', response)\n",
    "        request=None\n",
    "        current_message=None\n",
    "        response=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
