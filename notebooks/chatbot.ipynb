{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a simple chatbot using the ChatGPT model\n",
    "\n",
    "*__NOTE:__\n",
    "For instructions on running the Jupyter Notebook that contains the labs see instructions here: <https://github.com/retaildevcrews/OpenAI-Labs>*\n",
    "\n",
    "This sample notebook demonstrates  creation of a simple chatbot that uses the ChatGPT model using the Chat Completion API.  The API takes in an array of messages that have the following parts:\n",
    "- role (system - sets the overall behavior for the assistant , user - provides specific instructions for the assistant, assistant - stores previous responses and can be used for fine tuning to achieve desired behavior)\n",
    "- content\n",
    "\n",
    "For this lab the system message will be a simple description, and the messages sent to the api will be a collection that inclues all of the previous messages and responses.  In addition this example will track the number of tokens in the message to ensure it does not exceed the maximum.\n",
    "\n",
    "The chatbot labs are based on examples found in the <https://github.com/Azure/openai-samples> repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 09:26:00,830 name=ffmodel.core.inference_endpoint level=METRICS solution_config \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 3 configs from file '/Users/jorgeluna/.product-search.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n",
      "FFModelLogger: Logging will print to console as no AppInsight connection was provided (config `ApplicationInsightConnectionString)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jorgeluna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ffmodel.core.inference_endpoint import InferenceEndpoint\n",
    "\n",
    "environment_config_path = \"~/.chatbot.env\"\n",
    "\n",
    "#solution_config_path = \"../solution/configs/00_baseline.yaml\"\n",
    "#solution_config_path = \"../solution/configs/01_with_static_context_01.yaml\"\n",
    "#solution_config_path = \"../solution/configs/02_with_fewshots_01 _min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/03_with_fewshots_01.yaml\"\n",
    "solution_config_path = \"../solution/configs/05_with_fewshots_02_min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/04_with_fewshots_02.yaml\"\n",
    "\n",
    "endpoint = InferenceEndpoint(solution_config_path, environment_config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define Request to OpenAI API\n",
    "For these labs we are using the Azure OpenAI Chat Completion API, this is different than the Completions API.  Information about its usage can be found here:\n",
    "<https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions>\n",
    "The chat completions API takes a collection of messages to provide history and context of chat to the model.  This function takes a collection of messages which are objects that define a role and content.\n",
    "\n",
    "We also create a function that calculates the number of tokens received from the message collection so that we can keep track of message size and not exceed threshhold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(chat_request):\n",
    "    response = endpoint.execute(chat_request)\n",
    "    return response.model_output.completions[0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 The conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to pizzaai.  What can I get for you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 09:26:24,134 name=ffmodel.core.inference_endpoint level=METRICS final_data_model \n",
      "2023-10-15 09:26:24,135 name=ffmodel.core.inference_endpoint level=METRICS request_complete \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, but we do not offer candy at our pizza and wings restaurant. We specialize in pizza, wings, and drinks. Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 09:26:52,478 name=ffmodel.core.inference_endpoint level=METRICS final_data_model \n",
      "2023-10-15 09:26:52,479 name=ffmodel.core.inference_endpoint level=METRICS request_complete \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, but we do not offer pie at our pizza and wings restaurant. We specialize in pizza, wings, and drinks. Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 09:27:46,684 name=ffmodel.core.inference_endpoint level=METRICS final_data_model \n",
      "2023-10-15 09:27:46,685 name=ffmodel.core.inference_endpoint level=METRICS request_complete \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry, but I am unable to process that request as it is not a valid order for pizza, wings, or drinks. Please let me know if you would like to place an order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 09:28:15,279 name=ffmodel.core.inference_endpoint level=METRICS final_data_model \n",
      "2023-10-15 09:28:15,280 name=ffmodel.core.inference_endpoint level=METRICS request_complete \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  {\"items\": [{\"type\": \"drink\", \"name\": \"Sprite\", \"quantity\": 2, \"size\": \"2 liter\"}, {\"type\": \"custom pizza\", \"size\": \"large\", \"crust\": \"regular\", \"quantity\": 2, \"toppings\": [\"pepperoni\"]}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "\n",
    "class CompletionPair(BaseModel):\n",
    "    user_nl:str\n",
    "    completion:object\n",
    "\n",
    "class UserSession(BaseModel):\n",
    "    session_id:str\n",
    "    prior_responses:list[CompletionPair]\n",
    "    sequence: typing.Optional[int]\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_nl:str\n",
    "    session:UserSession\n",
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "max_response_tokens = 500\n",
    "token_limit=2048\n",
    "name = input('Enter Your Name: ')\n",
    "print ('Welcome to pizzaai.  What can I get for you?')\n",
    "conversation=[]\n",
    "session_id=str(uuid.uuid4())\n",
    "message_count=0\n",
    "while True:\n",
    "    request = input(name+':')\n",
    "    current_message={request}\n",
    "    chat_request=ChatRequest(user_nl=request,session=UserSession(session_id=session_id,sequence=message_count,prior_responses=conversation))\n",
    "    if request==\"Bye\" or request=='bye':\n",
    "        print('Bot: Bye')\n",
    "        break\n",
    "    else:\n",
    "        payload=json.dumps(jsonable_encoder(chat_request))\n",
    "        response = send_message(payload)\n",
    "        interaction = CompletionPair(user_nl=request,completion=response)\n",
    "        conversation.append(interaction)\n",
    "        message_count+=1\n",
    "        print('Bot: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
