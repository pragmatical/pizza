{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a simple chatbot using the ChatGPT model\n",
    "\n",
    "*__NOTE:__\n",
    "For instructions on running the Jupyter Notebook that contains the labs see instructions here: <https://github.com/retaildevcrews/OpenAI-Labs>*\n",
    "\n",
    "This sample notebook demonstrates  creation of a simple chatbot that uses the ChatGPT model using the Chat Completion API.  The API takes in an array of messages that have the following parts:\n",
    "- role (system - sets the overall behavior for the assistant , user - provides specific instructions for the assistant, assistant - stores previous responses and can be used for fine tuning to achieve desired behavior)\n",
    "- content\n",
    "\n",
    "For this lab the system message will be a simple description, and the messages sent to the api will be a collection that inclues all of the previous messages and responses.  In addition this example will track the number of tokens in the message to ensure it does not exceed the maximum.\n",
    "\n",
    "The chatbot labs are based on examples found in the <https://github.com/Azure/openai-samples> repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 10:12:22,893 name=ffmodel.core.inference_endpoint level=INFO Initializing inference runtime. \n",
      "2023-10-14 10:12:22,894 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-14 10:12:22,895 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-14 10:12:22,895 name=ffmodel.core.inference_endpoint level=METRICS solution_config \n",
      "2023-10-14 10:12:22,896 name=ffmodel.core.inference_endpoint level=INFO Inference runtime initialized. \n",
      "2023-10-14 10:12:22,896 name=ffmodel.core.inference_endpoint level=INFO Downloading supporting data for solution pipeline. \n",
      "2023-10-14 10:12:22,900 name=ffmodel.core.inference_endpoint level=INFO Downloaded file names: [] \n",
      "2023-10-14 10:12:22,901 name=ffmodel.core.inference_endpoint level=INFO Initializing solution pipeline. \n",
      "2023-10-14 10:12:22,925 name=ffmodel.core.inference_endpoint level=INFO Adding reader components.readers.text_prompt to the pipeline. \n",
      "2023-10-14 10:12:22,937 name=ffmodel.core.inference_endpoint level=INFO Adding component components.pre_processors.static_context_from_file to the pipeline. \n",
      "2023-10-14 10:12:22,944 name=ffmodel.core.inference_endpoint level=INFO Adding component components.pre_processors.few_shots_from_file to the pipeline. \n",
      "2023-10-14 10:12:22,950 name=ffmodel.core.inference_endpoint level=INFO Adding component components.stitchers.openai_chat_completions to the pipeline. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 2 configs from file '/Users/jorgeluna/.product-search.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n",
      "FFModelLogger: Logging will print to console as no AppInsight connection was provided (config `ApplicationInsightConnectionString)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 10:12:23,705 name=ffmodel.core.inference_endpoint level=INFO Adding component components.model_callers.openai_chat_completions to the pipeline. \n",
      "2023-10-14 10:12:23,713 name=ffmodel.core.inference_endpoint level=INFO Adding component components.post_processors.minify_json to the pipeline. \n",
      "2023-10-14 10:12:23,729 name=ffmodel.core.inference_endpoint level=INFO Skipping non-inference component components.evaluators.json_schema in the pipeline. \n",
      "2023-10-14 10:12:23,864 name=ffmodel.core.inference_endpoint level=INFO Skipping non-inference component components.evaluators.fuzzy in the pipeline. \n",
      "[nltk_data] Downloading package punkt to /Users/jorgeluna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2023-10-14 10:12:30,658 name=ffmodel.core.inference_endpoint level=INFO Skipping non-inference component components.evaluators.bleu in the pipeline. \n",
      "2023-10-14 10:12:31,057 name=ffmodel.core.inference_endpoint level=INFO Skipping non-inference component components.evaluators.rouge in the pipeline. \n",
      "2023-10-14 10:12:31,069 name=ffmodel.core.inference_endpoint level=INFO Adding writer components.writers.jsonl to the pipeline. \n",
      "2023-10-14 10:12:31,070 name=ffmodel.core.inference_endpoint level=INFO Successfully initialized solution pipeline with solution-id: 05 \n"
     ]
    }
   ],
   "source": [
    "from ffmodel.core.inference_endpoint import InferenceEndpoint\n",
    "\n",
    "environment_config_path = \"~/.product-search.env\"\n",
    "\n",
    "#solution_config_path = \"../solution/configs/00_baseline.yaml\"\n",
    "#solution_config_path = \"../solution/configs/01_with_static_context_01.yaml\"\n",
    "#solution_config_path = \"../solution/configs/02_with_fewshots_01 _min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/03_with_fewshots_01.yaml\"\n",
    "solution_config_path = \"../solution/configs/05_with_fewshots_02_min.yaml\"\n",
    "#solution_config_path = \"../solution/configs/04_with_fewshots_02.yaml\"\n",
    "\n",
    "endpoint = InferenceEndpoint(solution_config_path, environment_config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define Request to OpenAI API\n",
    "For these labs we are using the Azure OpenAI Chat Completion API, this is different than the Completions API.  Information about its usage can be found here:\n",
    "<https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions>\n",
    "The chat completions API takes a collection of messages to provide history and context of chat to the model.  This function takes a collection of messages which are objects that define a role and content.\n",
    "\n",
    "We also create a function that calculates the number of tokens received from the message collection so that we can keep track of message size and not exceed threshhold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(chat_request):\n",
    "    response = endpoint.execute(chat_request)\n",
    "    return response.model_output.completions[0]\n",
    "\n",
    "# Defining a function that counts the number of tokens\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 The conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to pizzaai.  What can I get for you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 10:20:14,860 name=ffmodel.core.inference_endpoint level=INFO Starting to process user prompt: user_nl='I would like some candy' session=UserSession(session_id='95333923-cae4-4c6f-8f79-03b9202489d8', prior_responses=[], sequence=0) \n",
      "2023-10-14 10:20:14,861 name=ffmodel.core.inference_endpoint level=INFO Executing components.pre_processors.static_context_from_file \n",
      "2023-10-14 10:20:14,862 name=ffmodel.core.inference_endpoint level=INFO Executing components.pre_processors.few_shots_from_file \n",
      "2023-10-14 10:20:14,862 name=ffmodel.core.inference_endpoint level=INFO Executing components.stitchers.openai_chat_completions \n",
      "2023-10-14 10:20:14,863 name=ffmodel.core.inference_endpoint level=ERROR components.stitchers.openai_chat_completions Component failed with an exception Object of type ChatRequest is not JSON serializable. \n",
      "2023-10-14 10:20:14,863 name=ffmodel.core.inference_endpoint level=ERROR Object of type ChatRequest is not JSON serializable \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jorgeluna/work/ffmodel-projects/ffmodel-3.10/lib/python3.10/site-packages/ffmodel/core/inference_endpoint.py\", line 180, in execute\n",
      "    data_model = step.execute(data_model)\n",
      "  File \"/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/../solution/components/stitchers/openai_chat_completions.py\", line 64, in execute\n",
      "    prompt = json.dumps(messages, indent=2)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 201, in encode\n",
      "    chunks = list(chunks)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 429, in _iterencode\n",
      "    yield from _iterencode_list(o, _current_indent_level)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\n",
      "    yield from chunks\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type ChatRequest is not JSON serializable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ChatRequest is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/work/ffmodel-projects/ffmodel-3.10/lib/python3.10/site-packages/ffmodel/core/inference_endpoint.py:180\u001b[0m, in \u001b[0;36mInferenceEndpoint.execute\u001b[0;34m(self, raw_request, request_id)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     data_model \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49mexecute(data_model)\n\u001b[1;32m    181\u001b[0m     DataModelLogger\u001b[39m.\u001b[39mlog_data_model(data_model, step\u001b[39m.\u001b[39mget_id())\n",
      "File \u001b[0;32m~/work/ffmodel-projects/pizza/notebooks/../solution/components/stitchers/openai_chat_completions.py:64\u001b[0m, in \u001b[0;36mComponent.execute\u001b[0;34m(self, data_model)\u001b[0m\n\u001b[1;32m     62\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: data_model\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39muser_nl})\n\u001b[0;32m---> 64\u001b[0m prompt \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mdumps(messages, indent\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     66\u001b[0m data_model\u001b[39m.\u001b[39mmodel_input\u001b[39m.\u001b[39mprompt \u001b[39m=\u001b[39m prompt\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(chunks)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:429\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39ma serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m(to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ChatRequest is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     payload\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mdumps(jsonable_encoder(chat_request))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     response \u001b[39m=\u001b[39m send_message(chat_request)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     interaction \u001b[39m=\u001b[39m CompletionPair(user_nl\u001b[39m=\u001b[39mrequest,completion\u001b[39m=\u001b[39mresponse)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     conversation\u001b[39m.\u001b[39mappend(interaction)\n",
      "\u001b[1;32m/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_message\u001b[39m(chat_request):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39;49mexecute(chat_request)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgeluna/work/ffmodel-projects/pizza/notebooks/chatbot.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mmodel_output\u001b[39m.\u001b[39mcompletions[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/work/ffmodel-projects/ffmodel-3.10/lib/python3.10/site-packages/ffmodel/core/inference_endpoint.py:187\u001b[0m, in \u001b[0;36mInferenceEndpoint.execute\u001b[0;34m(self, raw_request, request_id)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39merror(error_message, data_model\u001b[39m=\u001b[39mdata_model)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39mexception(e, data_model\u001b[39m=\u001b[39mdata_model)\n\u001b[0;32m--> 187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError Data Model:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mjson\u001b[39m.\u001b[39;49mdumps(data_model\u001b[39m.\u001b[39;49mto_dict())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, data_model\u001b[39m=\u001b[39mdata_model)\n\u001b[1;32m    188\u001b[0m DataModelLogger\u001b[39m.\u001b[39mlog_data_model(data_model, step\u001b[39m.\u001b[39mget_id())\n\u001b[1;32m    189\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_message)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ChatRequest is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "\n",
    "class CompletionPair(BaseModel):\n",
    "    user_nl:str\n",
    "    completion:object\n",
    "\n",
    "class UserSession(BaseModel):\n",
    "    session_id:str\n",
    "    prior_responses:list[CompletionPair]\n",
    "    sequence: typing.Optional[int]\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_nl:str\n",
    "    session:UserSession\n",
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "max_response_tokens = 500\n",
    "token_limit=2048\n",
    "name = input('Enter Your Name: ')\n",
    "print ('Welcome to pizzaai.  What can I get for you?')\n",
    "conversation=[]\n",
    "session_id=str(uuid.uuid4())\n",
    "message_count=0\n",
    "while True:\n",
    "    request = input(name+':')\n",
    "    current_message={request}\n",
    "    chat_request=ChatRequest(user_nl=request,session=UserSession(session_id=session_id,sequence=message_count,prior_responses=conversation))\n",
    "    conv_history_token_count = num_tokens_from_messages(conversation)\n",
    "    #reduce history to fall below limits\n",
    "    while (conv_history_token_count+max_response_tokens >= token_limit):\n",
    "        del conversation[1] \n",
    "        conv_history_token_count = num_tokens_from_messages(conversation)\n",
    "    if request==\"Bye\" or request=='bye':\n",
    "        print('Bot: Bye')\n",
    "        break\n",
    "    else:\n",
    "        payload=json.dumps(jsonable_encoder(chat_request))\n",
    "        response = send_message(payload)\n",
    "        interaction = CompletionPair(user_nl=request,completion=response)\n",
    "        conversation.append(interaction)\n",
    "        message_count+=1\n",
    "        print('sent token count: ' + str(conv_history_token_count))\n",
    "        print('Bot: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
